{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder .appName(\"WordCountApp\") .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neeli\\spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"sample.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.txt exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"sample.txt exists:\", os.path.isfile(\"sample.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'data.csv', 'output.avro', 'output.json', 'output.parquet', 'output.txt', 'sample.csv', 'sample.json', 'sample.txt', 'spark.ipynb', 'spark310env', 'venv', 'wordcount.py']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"C:/Users/neeli/spark/sample.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Pathname /C:/Users/neeli/spark/sample.txt from hdfs://localhost:9000/C:/Users/neeli/spark/sample.txt is not a valid DFS filename.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, count \u001b[38;5;129;01min\u001b[39;00m word_counts\u001b[38;5;241m.\u001b[39mcollect():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\neeli\\spark\\spark310env\\lib\\site-packages\\pyspark\\rdd.py:3552\u001b[0m, in \u001b[0;36mRDD.reduceByKey\u001b[1;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[0;32m   3505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreduceByKey\u001b[39m(\n\u001b[0;32m   3506\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3507\u001b[0m     func: Callable[[V, V], V],\n\u001b[0;32m   3508\u001b[0m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3509\u001b[0m     partitionFunc: Callable[[K], \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m portable_hash,\n\u001b[0;32m   3510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3512\u001b[0m \u001b[38;5;124;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[0;32m   3513\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3550\u001b[0m \u001b[38;5;124;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[0;32m   3551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neeli\\spark\\spark310env\\lib\\site-packages\\pyspark\\rdd.py:3975\u001b[0m, in \u001b[0;36mRDD.combineByKey\u001b[1;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[0;32m   3913\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3914\u001b[0m \u001b[38;5;124;03mGeneric function to combine the elements for each key using a custom\u001b[39;00m\n\u001b[0;32m   3915\u001b[0m \u001b[38;5;124;03mset of aggregation functions.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3972\u001b[0m \u001b[38;5;124;03m[('a', [1, 2]), ('b', [1])]\u001b[39;00m\n\u001b[0;32m   3973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numPartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3975\u001b[0m     numPartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaultReducePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3977\u001b[0m serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mserializer\n\u001b[0;32m   3978\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_limit()\n",
      "File \u001b[1;32mc:\\Users\\neeli\\spark\\spark310env\\lib\\site-packages\\pyspark\\rdd.py:4867\u001b[0m, in \u001b[0;36mRDD._defaultReducePartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mdefaultParallelism\n\u001b[0;32m   4866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\neeli\\spark\\spark310env\\lib\\site-packages\\pyspark\\rdd.py:5453\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m-> 5453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\neeli\\spark\\spark310env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\neeli\\spark\\spark310env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Pathname /C:/Users/neeli/spark/sample.txt from hdfs://localhost:9000/C:/Users/neeli/spark/sample.txt is not a valid DFS filename."
     ]
    }
   ],
   "source": [
    "\n",
    "word_counts = rdd.flatMap(lambda line: line.split()).map(lambda word: (word.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world: 1\n",
      "is: 1\n",
      "hello: 2\n",
      "spark: 2\n",
      "fast: 1\n"
     ]
    }
   ],
   "source": [
    "word_counts = rdd.flatMap(lambda line: line.split()) .map(lambda word: (word.strip().lower(), 1)) .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: 2\n",
      "world: 1\n",
      "spark: 2\n",
      "is: 1\n",
      "fast: 1\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"sample.json\")\n",
    "\n",
    "if df.count() > 0:\n",
    "    if 'text' in df.columns:\n",
    "        rdd = df.filter(df['text'].isNotNull()).rdd.flatMap(lambda row: str(row['text']).split()) .map(lambda word: (word.lower().strip(), 1)) .reduceByKey(lambda a, b: a + b)\n",
    "        \n",
    "        \n",
    "        for word, count in rdd.collect():\n",
    "            print(f\"{word}: {count}\")\n",
    "    else:\n",
    "        print(\"The 'text' column does not exist in the DataFrame.\")\n",
    "else:                  \n",
    "    print(\"The DataFrame is empty!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://harikiran:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>WordCountApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b2768ca050>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     hello world\n",
      "0    hello spark\n",
      "1  spark is fast\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  hello world|\n",
      "+-------------+\n",
      "|  hello spark|\n",
      "|spark is fast|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "spark_df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV successfully written to JSON, Parquet, Avro, and Text!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from fastavro import writer, parse_schema\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "df.to_json(\"output.json\", orient=\"records\", lines=True)\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, \"output.parquet\")\n",
    "\n",
    "\n",
    "df.to_csv(\"output.txt\", index=False, header=False)\n",
    "\n",
    "\n",
    "avro_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Record\",\n",
    "    \"fields\": [{\"name\": col, \"type\": \"string\"} for col in df.columns]\n",
    "}\n",
    "records = df.astype(str).to_dict(orient=\"records\")\n",
    "parsed_schema = parse_schema(avro_schema)\n",
    "\n",
    "with open(\"output.avro\", \"wb\") as f:\n",
    "    writer(f, parsed_schema, records)\n",
    "\n",
    "print(\" CSV successfully written to JSON, Parquet, Avro, and Text!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to JSON, AVRO, TEXT\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  hello world|\n",
      "+-------------+\n",
      "|  hello spark|\n",
      "|spark is fast|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Read CSV from HDFS\") .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\")  .getOrCreate()\n",
    "csv_path = \"hdfs://localhost:9000/user/hadoop/inputdata/data.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(csv_path)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").json(\"hdfs://localhost:9000/user/hadoop/fileoutput/json_output\")\n",
    "from pyspark.sql.functions import concat_ws\n",
    "df_text = df.select(concat_ws(\", \", *df.columns).alias(\"value\"))\n",
    "df_text.write.mode(\"overwrite\").text(\"hdfs://localhost:9000/user/hadoop/fileoutput/text_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+------+---------+------+\n",
      "|Country/Region|Confirmed|Deaths|Recovered|Active|\n",
      "+--------------+---------+------+---------+------+\n",
      "|         India|  1480073| 33408|   951166|495499|\n",
      "+--------------+---------+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read covid csv from HDFS\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "csv_path = \"hdfs://localhost:9000/user/hadoop/covid/country_wise_latest (1).csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "\n",
    "df.createOrReplaceTempView(\"covid_data\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, Confirmed, Deaths, Recovered, Active\n",
    "    FROM covid_data\n",
    "    where `Country/Region` = 'India'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+---------------+---------------------+\n",
      "|Country/Region|Total_Confirmed|Total_Recovered|Recovery_Rate_Percent|\n",
      "+--------------+---------------+---------------+---------------------+\n",
      "|      Dominica|             18|             18|                100.0|\n",
      "|       Grenada|             23|             23|                100.0|\n",
      "|      Holy See|             12|             12|                100.0|\n",
      "|      Djibouti|           5059|           4977|                98.38|\n",
      "|       Iceland|           1854|           1823|                98.33|\n",
      "|        Brunei|            141|            138|                97.87|\n",
      "|   New Zealand|           1557|           1514|                97.24|\n",
      "|         Qatar|         109597|         106328|                97.02|\n",
      "|      Malaysia|           8904|           8601|                 96.6|\n",
      "| Liechtenstein|             86|             83|                96.51|\n",
      "+--------------+---------------+---------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Top 10 Countries by COVID-19 Recovery Rates\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "csv_path = \"hdfs://localhost:9000/user/hadoop/covid/full_grouped.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "\n",
    "df.createOrReplaceTempView(\"full_grouped_data\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, \n",
    "           MAX(Confirmed) AS Total_Confirmed,\n",
    "           MAX(Recovered) AS Total_Recovered,\n",
    "           ROUND(MAX(Recovered) / MAX(Confirmed) * 100, 2) AS Recovery_Rate_Percent\n",
    "    FROM full_grouped_data\n",
    "    WHERE Confirmed > 0\n",
    "    GROUP BY `Country/Region`\n",
    "    ORDER BY Recovery_Rate_Percent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|     Country/Region|Confirmed|Deaths|Recovered|Active|New cases|New deaths|New recovered|Deaths / 100 Cases|Recovered / 100 Cases|Deaths / 100 Recovered|Confirmed last week|1 week change|1 week % increase|          WHO Region|\n",
      "+-------------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|        Afghanistan|    36263|  1269|    25198|  9796|      106|        10|           18|               3.5|                69.49|                  5.04|              35526|          737|             2.07|Eastern Mediterra...|\n",
      "|            Albania|     4880|   144|     2745|  1991|      117|         6|           63|              2.95|                56.25|                  5.25|               4171|          709|             17.0|              Europe|\n",
      "|            Algeria|    27973|  1163|    18837|  7973|      616|         8|          749|              4.16|                67.34|                  6.17|              23691|         4282|            18.07|              Africa|\n",
      "|            Andorra|      907|    52|      803|    52|       10|         0|            0|              5.73|                88.53|                  6.48|                884|           23|              2.6|              Europe|\n",
      "|             Angola|      950|    41|      242|   667|       18|         1|            0|              4.32|                25.47|                 16.94|                749|          201|            26.84|              Africa|\n",
      "|Antigua and Barbuda|       86|     3|       65|    18|        4|         0|            5|              3.49|                75.58|                  4.62|                 76|           10|            13.16|            Americas|\n",
      "|          Argentina|   167416|  3059|    72575| 91782|     4890|       120|         2057|              1.83|                43.35|                  4.21|             130774|        36642|            28.02|            Americas|\n",
      "|            Armenia|    37390|   711|    26665| 10014|       73|         6|          187|               1.9|                71.32|                  2.67|              34981|         2409|             6.89|              Europe|\n",
      "|          Australia|    15303|   167|     9311|  5825|      368|         6|          137|              1.09|                60.84|                  1.79|              12428|         2875|            23.13|     Western Pacific|\n",
      "|            Austria|    20558|   713|    18246|  1599|       86|         1|           37|              3.47|                88.75|                  3.91|              19743|          815|             4.13|              Europe|\n",
      "|         Azerbaijan|    30446|   423|    23242|  6781|      396|         6|          558|              1.39|                76.34|                  1.82|              27890|         2556|             9.16|              Europe|\n",
      "|            Bahamas|      382|    11|       91|   280|       40|         0|            0|              2.88|                23.82|                 12.09|                174|          208|           119.54|            Americas|\n",
      "|            Bahrain|    39482|   141|    36110|  3231|      351|         1|          421|              0.36|                91.46|                  0.39|              36936|         2546|             6.89|Eastern Mediterra...|\n",
      "|         Bangladesh|   226225|  2965|   125683| 97577|     2772|        37|         1801|              1.31|                55.56|                  2.36|             207453|        18772|             9.05|     South-East Asia|\n",
      "|           Barbados|      110|     7|       94|     9|        0|         0|            0|              6.36|                85.45|                  7.45|                106|            4|             3.77|            Americas|\n",
      "|            Belarus|    67251|   538|    60492|  6221|      119|         4|           67|               0.8|                89.95|                  0.89|              66213|         1038|             1.57|              Europe|\n",
      "|            Belgium|    66428|  9822|    17452| 39154|      402|         1|           14|             14.79|                26.27|                 56.28|              64094|         2334|             3.64|              Europe|\n",
      "|             Belize|       48|     2|       26|    20|        0|         0|            0|              4.17|                54.17|                  7.69|                 40|            8|             20.0|            Americas|\n",
      "|              Benin|     1770|    35|     1036|   699|        0|         0|            0|              1.98|                58.53|                  3.38|               1602|          168|            10.49|              Africa|\n",
      "|             Bhutan|       99|     0|       86|    13|        4|         0|            1|               0.0|                86.87|                   0.0|                 90|            9|             10.0|     South-East Asia|\n",
      "+-------------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------+---------+----------+----------+---------+------+---------+------+--------------------+\n",
      "|      Province/State|     Country/Region|      Lat|      Long|      Date|Confirmed|Deaths|Recovered|Active|          WHO Region|\n",
      "+--------------------+-------------------+---------+----------+----------+---------+------+---------+------+--------------------+\n",
      "|                NULL|        Afghanistan| 33.93911| 67.709953|2020-01-22|        0|     0|        0|     0|Eastern Mediterra...|\n",
      "|                NULL|            Albania|  41.1533|   20.1683|2020-01-22|        0|     0|        0|     0|              Europe|\n",
      "|                NULL|            Algeria|  28.0339|    1.6596|2020-01-22|        0|     0|        0|     0|              Africa|\n",
      "|                NULL|            Andorra|  42.5063|    1.5218|2020-01-22|        0|     0|        0|     0|              Europe|\n",
      "|                NULL|             Angola| -11.2027|   17.8739|2020-01-22|        0|     0|        0|     0|              Africa|\n",
      "|                NULL|Antigua and Barbuda|  17.0608|  -61.7964|2020-01-22|        0|     0|        0|     0|            Americas|\n",
      "|                NULL|          Argentina| -38.4161|  -63.6167|2020-01-22|        0|     0|        0|     0|            Americas|\n",
      "|                NULL|            Armenia|  40.0691|   45.0382|2020-01-22|        0|     0|        0|     0|              Europe|\n",
      "|Australian Capita...|          Australia| -35.4735|  149.0124|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|     New South Wales|          Australia| -33.8688|  151.2093|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|  Northern Territory|          Australia| -12.4634|  130.8456|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|          Queensland|          Australia| -27.4698|  153.0251|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|     South Australia|          Australia| -34.9285|  138.6007|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|            Tasmania|          Australia| -42.8821|  147.3272|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|            Victoria|          Australia| -37.8136|  144.9631|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|   Western Australia|          Australia| -31.9505|  115.8605|2020-01-22|        0|     0|        0|     0|     Western Pacific|\n",
      "|                NULL|            Austria|  47.5162|   14.5501|2020-01-22|        0|     0|        0|     0|              Europe|\n",
      "|                NULL|         Azerbaijan|  40.1431|   47.5769|2020-01-22|        0|     0|        0|     0|              Europe|\n",
      "|                NULL|            Bahamas|25.025885|-78.035889|2020-01-22|        0|     0|        0|     0|            Americas|\n",
      "|                NULL|            Bahrain|  26.0275|     50.55|2020-01-22|        0|     0|        0|     0|Eastern Mediterra...|\n",
      "+--------------------+-------------------+---------+----------+----------+---------+------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----+----+----+-----+----+---+---+----+----+------+---+\n",
      "|22-01-2020|  555|  17|  28|  510|  05| 06| 07|3.06|5.05| 60.71|  6|\n",
      "+----------+-----+----+----+-----+----+---+---+----+----+------+---+\n",
      "|23-01-2020|  654|  18|  30|  606|  99|  1|  2|2.75|4.59|  60.0|  8|\n",
      "|24-01-2020|  941|  26|  36|  879| 287|  8|  6|2.76|3.83| 72.22|  9|\n",
      "|25-01-2020| 1434|  42|  39| 1353| 493| 16|  3|2.93|2.72|107.69| 11|\n",
      "|26-01-2020| 2118|  56|  52| 2010| 684| 14| 13|2.64|2.46|107.69| 13|\n",
      "|27-01-2020| 2927|  82|  61| 2784| 809| 26|  9| 2.8|2.08|134.43| 16|\n",
      "|28-01-2020| 5578| 131| 107| 5340|2651| 49| 46|2.35|1.92|122.43| 16|\n",
      "|29-01-2020| 6166| 133| 125| 5908| 588|  2| 18|2.16|2.03| 106.4| 18|\n",
      "|30-01-2020| 8234| 171| 141| 7922|2068| 38| 16|2.08|1.71|121.28| 20|\n",
      "|31-01-2020| 9927| 213| 219| 9495|1693| 42| 78|2.15|2.21| 97.26| 24|\n",
      "|01-02-2020|12038| 259| 281|11498|2111| 46| 62|2.15|2.33| 92.17| 25|\n",
      "|02-02-2020|16787| 362| 459|15966|4749|103|178|2.16|2.73| 78.87| 25|\n",
      "|03-02-2020|19887| 426| 604|18857|3100| 64|145|2.14|3.04| 70.53| 25|\n",
      "|04-02-2020|23898| 492| 821|22585|4011| 66|217|2.06|3.44| 59.93| 26|\n",
      "|05-02-2020|27643| 564|1071|26008|3745| 72|250|2.04|3.87| 52.66| 26|\n",
      "|06-02-2020|30802| 634|1418|28750|3159| 70|347|2.06| 4.6| 44.71| 26|\n",
      "|07-02-2020|34334| 719|1903|31712|3532| 85|485|2.09|5.54| 37.78| 26|\n",
      "|08-02-2020|37068| 806|2470|33792|2734| 87|567|2.17|6.66| 32.63| 26|\n",
      "|09-02-2020|40095| 906|3057|36132|3027|100|587|2.26|7.62| 29.64| 26|\n",
      "|10-02-2020|42633|1013|3714|37906|2538|107|657|2.38|8.71| 27.28| 26|\n",
      "|11-02-2020|44675|1113|4417|39145|2042|100|703|2.49|9.89|  25.2| 26|\n",
      "+----------+-----+----+----+-----+----+---+---+----+----+------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-------------------+---------+------+---------+------+---------+----------+-------------+--------------------+\n",
      "|      Date|     Country/Region|Confirmed|Deaths|Recovered|Active|New cases|New deaths|New recovered|          WHO Region|\n",
      "+----------+-------------------+---------+------+---------+------+---------+----------+-------------+--------------------+\n",
      "|2020-01-22|        Afghanistan|        0|     0|        0|     0|        0|         0|            0|Eastern Mediterra...|\n",
      "|2020-01-22|            Albania|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
      "|2020-01-22|            Algeria|        0|     0|        0|     0|        0|         0|            0|              Africa|\n",
      "|2020-01-22|            Andorra|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
      "|2020-01-22|             Angola|        0|     0|        0|     0|        0|         0|            0|              Africa|\n",
      "|2020-01-22|Antigua and Barbuda|        0|     0|        0|     0|        0|         0|            0|            Americas|\n",
      "|2020-01-22|          Argentina|        0|     0|        0|     0|        0|         0|            0|            Americas|\n",
      "|2020-01-22|            Armenia|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
      "|2020-01-22|          Australia|        0|     0|        0|     0|        0|         0|            0|     Western Pacific|\n",
      "|2020-01-22|            Austria|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
      "|2020-01-22|         Azerbaijan|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
      "|2020-01-22|            Bahamas|        0|     0|        0|     0|        0|         0|            0|            Americas|\n",
      "|2020-01-22|            Bahrain|        0|     0|        0|     0|        0|         0|            0|Eastern Mediterra...|\n",
      "|2020-01-22|         Bangladesh|        0|     0|        0|     0|        0|         0|            0|     South-East Asia|\n",
      "|2020-01-22|           Barbados|        0|     0|        0|     0|        0|         0|            0|            Americas|\n",
      "|2020-01-22|            Belarus|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
      "|2020-01-22|            Belgium|        0|     0|        0|     0|        0|         0|            0|              Europe|\n",
      "|2020-01-22|             Belize|        0|     0|        0|     0|        0|         0|            0|            Americas|\n",
      "|2020-01-22|              Benin|        0|     0|        0|     0|        0|         0|            0|              Africa|\n",
      "|2020-01-22|             Bhutan|        0|     0|        0|     0|        0|         0|            0|     South-East Asia|\n",
      "+----------+-------------------+---------+------+---------+------+---------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----+----+-----+-------+------------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "|     UID|iso2|iso3|code3|   FIPS|      Admin2|      Province_State|Country_Region|                Lat|             Long_|        Combined_Key|   Date|Confirmed|Deaths|\n",
      "+--------+----+----+-----+-------+------------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "|      16|  AS| ASM|   16|   60.0|        NULL|      American Samoa|            US|-14.270999999999999|          -170.132|  American Samoa, US|1/22/20|        0|     0|\n",
      "|     316|  GU| GUM|  316|   66.0|        NULL|                Guam|            US|            13.4443|          144.7937|            Guam, US|1/22/20|        0|     0|\n",
      "|     580|  MP| MNP|  580|   69.0|        NULL|Northern Mariana ...|            US|            15.0979|          145.6739|Northern Mariana ...|1/22/20|        0|     0|\n",
      "|63072001|  PR| PRI|  630|72001.0|    Adjuntas|         Puerto Rico|            US| 18.180117000000006|        -66.754367|Adjuntas, Puerto ...|1/22/20|        0|     0|\n",
      "|63072003|  PR| PRI|  630|72003.0|      Aguada|         Puerto Rico|            US|          18.360255|-67.17513100000001|Aguada, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072005|  PR| PRI|  630|72005.0|   Aguadilla|         Puerto Rico|            US|          18.459681|-67.12081500000001|Aguadilla, Puerto...|1/22/20|        0|     0|\n",
      "|63072007|  PR| PRI|  630|72007.0|Aguas Buenas|         Puerto Rico|            US|          18.251619|        -66.126806|Aguas Buenas, Pue...|1/22/20|        0|     0|\n",
      "|63072009|  PR| PRI|  630|72009.0|    Aibonito|         Puerto Rico|            US|          18.131361|        -66.264131|Aibonito, Puerto ...|1/22/20|        0|     0|\n",
      "|63072011|  PR| PRI|  630|72011.0|      Anasco|         Puerto Rico|            US|          18.287985|        -67.120611|Anasco, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072013|  PR| PRI|  630|72013.0|     Arecibo|         Puerto Rico|            US|          18.406631|        -66.675077|Arecibo, Puerto R...|1/22/20|        0|     0|\n",
      "|63072015|  PR| PRI|  630|72015.0|      Arroyo|         Puerto Rico|            US| 17.998457000000005|        -66.056546|Arroyo, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072017|  PR| PRI|  630|72017.0| Barceloneta|         Puerto Rico|            US| 18.445532999999998|-66.56053100000001|Barceloneta, Puer...|1/22/20|        0|     0|\n",
      "|63072019|  PR| PRI|  630|72019.0|Barranquitas|         Puerto Rico|            US|          18.201592|         -66.30963|Barranquitas, Pue...|1/22/20|        0|     0|\n",
      "|63072021|  PR| PRI|  630|72021.0|     Bayamon|         Puerto Rico|            US|           18.34946|        -66.168435|Bayamon, Puerto R...|1/22/20|        0|     0|\n",
      "|63072023|  PR| PRI|  630|72023.0|   Cabo Rojo|         Puerto Rico|            US|          18.040993|        -67.154391|Cabo Rojo, Puerto...|1/22/20|        0|     0|\n",
      "|63072025|  PR| PRI|  630|72025.0|      Caguas|         Puerto Rico|            US|          18.211615|        -66.050779|Caguas, Puerto Ri...|1/22/20|        0|     0|\n",
      "|63072027|  PR| PRI|  630|72027.0|       Camuy|         Puerto Rico|            US|          18.418578|        -66.860206|Camuy, Puerto Ric...|1/22/20|        0|     0|\n",
      "|63072029|  PR| PRI|  630|72029.0|   Canovanas|         Puerto Rico|            US|          18.328802|        -65.887612|Canovanas, Puerto...|1/22/20|        0|     0|\n",
      "|63072031|  PR| PRI|  630|72031.0|    Carolina|         Puerto Rico|            US|          18.374986|-65.95683100000001|Carolina, Puerto ...|1/22/20|        0|     0|\n",
      "|63072033|  PR| PRI|  630|72033.0|      Catano|         Puerto Rico|            US|          18.437269|-66.14330600000001|Catano, Puerto Ri...|1/22/20|        0|     0|\n",
      "+--------+----+----+-----+-------+------------+--------------------+--------------+-------------------+------------------+--------------------+-------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------------+\n",
      "|Country/Region|    Continent|Population|TotalCases|NewCases|TotalDeaths|NewDeaths|TotalRecovered|NewRecovered|ActiveCases|Serious,Critical|Tot Cases/1M pop|Deaths/1M pop|TotalTests|Tests/1M pop|          WHO Region|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------------+\n",
      "|           USA|North America| 331198130|   5032179|    NULL|     162804|     NULL|       2576668|        NULL|    2292707|           18296|           15194|        492.0|  63139605|      190640|            Americas|\n",
      "|        Brazil|South America| 212710692|   2917562|    NULL|      98644|     NULL|       2047660|        NULL|     771258|            8318|           13716|        464.0|  13206188|       62085|            Americas|\n",
      "|         India|         Asia|1381344997|   2025409|    NULL|      41638|     NULL|       1377384|        NULL|     606387|            8944|            1466|         30.0|  22149351|       16035|      South-EastAsia|\n",
      "|        Russia|       Europe| 145940924|    871894|    NULL|      14606|     NULL|        676357|        NULL|     180931|            2300|            5974|        100.0|  29716907|      203623|              Europe|\n",
      "|  South Africa|       Africa|  59381566|    538184|    NULL|       9604|     NULL|        387316|        NULL|     141264|             539|            9063|        162.0|   3149807|       53044|              Africa|\n",
      "|        Mexico|North America| 129066160|    462690|    6590|      50517|      819|        308848|        4140|     103325|            3987|            3585|        391.0|   1056915|        8189|            Americas|\n",
      "|          Peru|South America|  33016319|    455409|    NULL|      20424|     NULL|        310337|        NULL|     124648|            1426|           13793|        619.0|   2493429|       75521|            Americas|\n",
      "|         Chile|South America|  19132514|    366671|    NULL|       9889|     NULL|        340168|        NULL|      16614|            1358|           19165|        517.0|   1760615|       92022|            Americas|\n",
      "|      Colombia|South America|  50936262|    357710|    NULL|      11939|     NULL|        192355|        NULL|     153416|            1493|            7023|        234.0|   1801835|       35374|            Americas|\n",
      "|         Spain|       Europe|  46756648|    354530|    NULL|      28500|     NULL|          NULL|        NULL|       NULL|             617|            7582|        610.0|   7064329|      151087|              Europe|\n",
      "|          Iran|         Asia|  84097623|    320117|    NULL|      17976|     NULL|        277463|        NULL|      24678|            4156|            3806|        214.0|   2612763|       31068|EasternMediterranean|\n",
      "|            UK|       Europe|  67922029|    308134|    NULL|      46413|     NULL|          NULL|        NULL|       NULL|              73|            4537|        683.0|  17515234|      257873|              Europe|\n",
      "|  Saudi Arabia|         Asia|  34865919|    284226|    NULL|       3055|     NULL|        247089|        NULL|      34082|            1915|            8152|         88.0|   3635705|      104277|EasternMediterranean|\n",
      "|      Pakistan|         Asia| 221295851|    281863|    NULL|       6035|     NULL|        256058|        NULL|      19770|             809|            1274|         27.0|   2058872|        9304|EasternMediterranean|\n",
      "|    Bangladesh|         Asia| 164851401|    249651|    NULL|       3306|     NULL|        143824|        NULL|     102521|            NULL|            1514|         20.0|   1225124|        7432|      South-EastAsia|\n",
      "|         Italy|       Europe|  60452568|    249204|    NULL|      35187|     NULL|        201323|        NULL|      12694|              42|            4122|        582.0|   7099713|      117443|              Europe|\n",
      "|        Turkey|         Asia|  84428331|    237265|    NULL|       5798|     NULL|        220546|        NULL|      10921|             580|            2810|         69.0|   5081802|       60191|              Europe|\n",
      "|     Argentina|South America|  45236884|    228195|    NULL|       4251|     NULL|         99852|        NULL|     124092|            1150|            5044|         94.0|    794544|       17564|            Americas|\n",
      "|       Germany|       Europe|  83811260|    215210|    NULL|       9252|     NULL|        196200|        NULL|       9758|             236|            2568|        110.0|   8586648|      102452|              Europe|\n",
      "|        France|       Europe|  65288306|    195633|    NULL|      30312|     NULL|         82460|        NULL|      82861|             384|            2996|        464.0|   3992206|       61147|              Europe|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read COVID CSV Files from HDFS\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_country_latest = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")  .csv(\"hdfs://localhost:9000/user/hadoop/covid/country_wise_latest (1).csv\")\n",
    "\n",
    "df_covid_clean = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") .csv(\"hdfs://localhost:9000/user/hadoop/covid/covid_19_clean_complete.csv\")\n",
    "\n",
    "df_day_wise = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") .csv(\"hdfs://localhost:9000/user/hadoop/covid/day_wise.csv\")\n",
    "\n",
    "df_full_grouped = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") .csv(\"hdfs://localhost:9000/user/hadoop/covid/full_grouped.csv\")\n",
    "\n",
    "df_usa_county = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") .csv(\"hdfs://localhost:9000/user/hadoop/covid/usa_county_wise.csv\")\n",
    "\n",
    "df_worldometer = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") .csv(\"hdfs://localhost:9000/user/hadoop/covid/worldometer_data.csv\")\n",
    "\n",
    "df_country_latest.show()\n",
    "df_covid_clean.show()\n",
    "df_day_wise.show()\n",
    "df_full_grouped.show()\n",
    "df_usa_county.show()\n",
    "df_worldometer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------+\n",
      "|Country/Region|Recovery_Rate|Fatality_Rate|\n",
      "+--------------+-------------+-------------+\n",
      "|      Dominica|        100.0|          0.0|\n",
      "|       Grenada|        100.0|          0.0|\n",
      "|      Holy See|        100.0|          0.0|\n",
      "|      Djibouti|        98.38|         1.15|\n",
      "|       Iceland|        98.33|         0.54|\n",
      "|        Brunei|        97.87|         2.13|\n",
      "|   New Zealand|        97.24|         1.41|\n",
      "|         Qatar|        97.02|         0.15|\n",
      "|      Malaysia|         96.6|         1.39|\n",
      "|     Mauritius|        96.51|         2.91|\n",
      "|        Norway|        95.84|         2.79|\n",
      "|       Taiwan*|        95.24|         1.52|\n",
      "|          Laos|         95.0|          0.0|\n",
      "|         Malta|        94.86|         1.28|\n",
      "|       Estonia|        94.54|         3.39|\n",
      "|      Thailand|        94.36|         1.76|\n",
      "| Liechtenstein|        94.19|         1.16|\n",
      "|    San Marino|        93.99|         6.01|\n",
      "|       Finland|        93.54|         4.45|\n",
      "|     Greenland|        92.86|          0.0|\n",
      "+--------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_latest.createOrReplaceTempView(\"country_latest\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, \n",
    "           ROUND(`Recovered / 100 Cases`, 2) AS Recovery_Rate,\n",
    "           ROUND(`Deaths / 100 Cases`, 2) AS Fatality_Rate\n",
    "    FROM country_latest\n",
    "    WHERE `Confirmed` > 0\n",
    "    ORDER BY Recovery_Rate DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|      Country/Region|Deaths|\n",
      "+--------------------+------+\n",
      "|              Bhutan|     0|\n",
      "|            Cambodia|     0|\n",
      "|            Dominica|     0|\n",
      "|             Eritrea|     0|\n",
      "|                Fiji|     0|\n",
      "|           Greenland|     0|\n",
      "|             Grenada|     0|\n",
      "|            Holy See|     0|\n",
      "|                Laos|     0|\n",
      "|            Mongolia|     0|\n",
      "|    Papua New Guinea|     0|\n",
      "|Saint Kitts and N...|     0|\n",
      "|         Saint Lucia|     0|\n",
      "|Saint Vincent and...|     0|\n",
      "|          Seychelles|     0|\n",
      "|         Timor-Leste|     0|\n",
      "|             Vietnam|     0|\n",
      "|             Burundi|     1|\n",
      "|       Liechtenstein|     1|\n",
      "|      Western Sahara|     1|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_latest.createOrReplaceTempView(\"country_latest\")\n",
    "\n",
    "result=spark.sql(\"\"\"\n",
    "SELECT `Country/Region`,Deaths\n",
    "from country_latest\n",
    "ORDER BY Deaths \n",
    "                 \n",
    "\"\"\")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|Country/Region|TotalCases|\n",
      "+--------------+----------+\n",
      "|           USA|   5032179|\n",
      "|        Brazil|   2917562|\n",
      "|         India|   2025409|\n",
      "|        Russia|    871894|\n",
      "|  South Africa|    538184|\n",
      "|        Mexico|    462690|\n",
      "|          Peru|    455409|\n",
      "|         Chile|    366671|\n",
      "|      Colombia|    357710|\n",
      "|         Spain|    354530|\n",
      "|          Iran|    320117|\n",
      "|            UK|    308134|\n",
      "|  Saudi Arabia|    284226|\n",
      "|      Pakistan|    281863|\n",
      "|    Bangladesh|    249651|\n",
      "|         Italy|    249204|\n",
      "|        Turkey|    237265|\n",
      "|     Argentina|    228195|\n",
      "|       Germany|    215210|\n",
      "|        France|    195633|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_worldometer.createOrReplaceTempView(\"country_latest\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "   SELECT `Country/Region`,TotalCases\n",
    "from country_latest\n",
    "ORDER BY TotalCases desc\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Global_Recovery_Rate|\n",
      "+--------------------+\n",
      "|               62.97|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_worldometer.createOrReplaceTempView(\"country_latest\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ROUND((SUM(TotalRecovered) / SUM(TotalCases)) * 100, 2) AS Global_Recovery_Rate\n",
    "    FROM country_latest\n",
    "    WHERE TotalCases > 0\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+------------+---------------+------------+-------------+-------------+\n",
      "|        Continent|Total_Confirmed|Total_Deaths|Total_Recovered|Total_Active|Recovery_Rate|Fatality_Rate|\n",
      "+-----------------+---------------+------------+---------------+------------+-------------+-------------+\n",
      "|    North America|        5919209|      229855|        3151678|     2537676|        53.24|         3.88|\n",
      "|             Asia|        4689794|      100627|        3508170|     1080997|         74.8|         2.15|\n",
      "|    South America|        4543273|      154885|        3116150|     1272238|        68.59|         3.41|\n",
      "|           Europe|        2982576|      205232|        1587302|      475261|        53.22|         6.88|\n",
      "|           Africa|        1011867|       22114|         693620|      296133|        68.55|         2.19|\n",
      "|Australia/Oceania|          21735|         281|          12620|        8834|        58.06|         1.29|\n",
      "|             NULL|            712|          13|            651|          48|        91.43|         1.83|\n",
      "+-----------------+---------------+------------+---------------+------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_worldometer.createOrReplaceTempView(\"country_latest\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Continent,\n",
    "        SUM(TotalCases) AS Total_Confirmed,\n",
    "        SUM(TotalDeaths) AS Total_Deaths,\n",
    "        SUM(TotalRecovered) AS Total_Recovered,\n",
    "        SUM(ActiveCases) AS Total_Active,\n",
    "        ROUND((SUM(TotalRecovered) / SUM(TotalCases)) * 100, 2) AS Recovery_Rate,\n",
    "        ROUND((SUM(TotalDeaths) / SUM(TotalCases)) * 100, 2) AS Fatality_Rate\n",
    "    FROM country_latest\n",
    "    WHERE TotalCases > 0\n",
    "    GROUP BY Continent\n",
    "    ORDER BY Total_Confirmed DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|        Continent|Total_Confirmed|\n",
      "+-----------------+---------------+\n",
      "|Australia/Oceania|          21735|\n",
      "+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_worldometer.createOrReplaceTempView(\"country_latest\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Continent,\n",
    "        SUM(TotalCases) AS Total_Confirmed\n",
    "    FROM country_latest\n",
    "    WHERE TotalCases > 0 AND Continent IS NOT NULL\n",
    "    GROUP BY Continent\n",
    "    ORDER BY Total_Confirmed ASC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+-----------+-------------+\n",
      "|        Continent|Total_Recovered|Total_Cases|Recovery_Rate|\n",
      "+-----------------+---------------+-----------+-------------+\n",
      "|             Asia|        3508170|    4689794|         74.8|\n",
      "|    South America|        3116150|    4543273|        68.59|\n",
      "|           Africa|         693620|    1011867|        68.55|\n",
      "|Australia/Oceania|          12620|      21735|        58.06|\n",
      "|    North America|        3151678|    5919209|        53.24|\n",
      "|           Europe|        1587302|    2982576|        53.22|\n",
      "+-----------------+---------------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_worldometer.createOrReplaceTempView(\"country_latest\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Continent,\n",
    "        SUM(TotalRecovered) AS Total_Recovered,\n",
    "        SUM(TotalCases) AS Total_Cases,\n",
    "        ROUND((SUM(TotalRecovered) / SUM(TotalCases)) * 100, 2) AS Recovery_Rate\n",
    "    FROM country_latest\n",
    "    WHERE TotalCases > 0 AND Continent IS NOT NULL\n",
    "    GROUP BY Continent\n",
    "    ORDER BY Recovery_Rate DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+----------------------+-----------------------+\n",
      "|       Country|TotalCases|TotalDeaths|Local_Death_Percentage|Global_Death_Percentage|\n",
      "+--------------+----------+-----------+----------------------+-----------------------+\n",
      "|         Yemen|      1768|        508|                 28.73|                   3.67|\n",
      "|        France|    195633|      30312|                 15.49|                   3.67|\n",
      "|         Italy|    249204|      35187|                 14.12|                   3.67|\n",
      "|       Belgium|     71158|       9859|                 13.86|                   3.67|\n",
      "|       Hungary|      4597|        600|                 13.05|                   3.67|\n",
      "|        Mexico|    462690|      50517|                 10.92|                   3.67|\n",
      "|   Netherlands|     56982|       6153|                  10.8|                   3.67|\n",
      "|Western Sahara|        10|          1|                  10.0|                   3.67|\n",
      "|          Chad|       942|         76|                  8.07|                   3.67|\n",
      "|         Spain|    354530|      28500|                  8.04|                   3.67|\n",
      "+--------------+----------+-----------+----------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_latest.createOrReplaceTempView(\"country_latest\")\n",
    "df_worldometer.createOrReplaceTempView(\"worldometer\")\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH joined_data AS (\n",
    "        SELECT \n",
    "            cl.`Country/Region` AS Country,\n",
    "            wm.TotalCases,\n",
    "            wm.TotalDeaths\n",
    "        FROM country_latest cl\n",
    "        JOIN worldometer wm\n",
    "        ON cl.`Country/Region` = wm.`Country/Region`\n",
    "        WHERE wm.TotalCases > 0 AND wm.TotalDeaths IS NOT NULL\n",
    "    )\n",
    "\n",
    "    SELECT \n",
    "        Country,\n",
    "        TotalCases,\n",
    "        TotalDeaths,\n",
    "        ROUND((TotalDeaths / TotalCases) * 100, 2) AS Local_Death_Percentage,\n",
    "        ROUND((SUM(TotalDeaths) OVER () / SUM(TotalCases) OVER ()) * 100, 2) AS Global_Death_Percentage\n",
    "    FROM joined_data\n",
    "    ORDER BY Local_Death_Percentage DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Spark)",
   "language": "python",
   "name": "spark310env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
